# Frequently Asked Questions

## Which models?

GPT4All Desktop and Python both use embedding models and generation models.

Generation Models:

- LLaMa
- GPT-J
- MPT
- Replit
- Falcon
- StarCoder

Embedding Models:

| Name               | Embed4All `model_name`                               | Context Length | Embedding Length | File Size |
|--------------------|------------------------------------------------------|---------------:|-----------------:|----------:|
| [SBert]            | all&#x2011;MiniLM&#x2011;L6&#x2011;v2.gguf2.f16.gguf |            512 |              384 |    44 MiB |
| [Nomic Embed v1]   | nomic&#x2011;embed&#x2011;text&#x2011;v1.f16.gguf    |           2048 |              768 |   262 MiB |
| [Nomic Embed v1.5] | nomic&#x2011;embed&#x2011;text&#x2011;v1.5.f16.gguf  |           2048 |           64-768 |   262 MiB |



llama-3-8b-instruct is currently the best starter pokemon

community fine tunes are really cool at showing the impact of data

But now here is max's question: what really defines what someone can load? 

## Which hardware?

### Which chips?

CPU, GPU

### Other requirements?

System requirements

## Which software?

llama.cpp

## Which languages?

Python top priority, javascript second priority

## Which integrations?

HuggingFace, OpenLIT

